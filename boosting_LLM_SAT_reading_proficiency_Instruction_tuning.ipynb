{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment"
      ],
      "metadata": {
        "id": "8O-0ts6iyni9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE0dzJA1yAAu"
      },
      "outputs": [],
      "source": [
        "!pip install-q-U bitsandbytes\n",
        "!pip install-q-U datasets\n",
        "!pip install-q-U git+https://github.com/huggingface/transformers.git\n",
        "!pip install-q-U git+https://github.com/huggingface/peft.git\n",
        "!pip install-q-U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install-q-U loralib\n",
        "!pip install-q-U einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import bitsandbytes as bnd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    PeftModel,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig\n",
        ")"
      ],
      "metadata": {
        "id": "xz2pHGIxyA4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Base Model"
      ],
      "metadata": {
        "id": "Z6q5pAUQyryU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "token = input('Input token: ')\n",
        "login(token=token)"
      ],
      "metadata": {
        "id": "tdc2z9ZXyA16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "8_eBlzh5yAzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure QLoRA"
      ],
      "metadata": {
        "id": "c6HUrhMF0fna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\n",
        "        'q_proj',\n",
        "        'v_proj'\n",
        "    ],\n",
        "    lora_dropout=0.05,\n",
        "    bias='none',\n",
        "    task_type='CAUSAL_LM'\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "DtF6VvkXyAwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset"
      ],
      "metadata": {
        "id": "Thzvi2HI05Sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset('emozilla/sat-reading')\n",
        "\n",
        "def extract_sections(text):\n",
        "    sections = {\n",
        "        'passage': '',\n",
        "        'question': '',\n",
        "        'choices': [],\n",
        "        'answer_letter': ''\n",
        "    }\n",
        "\n",
        "    answer_part = text.split('Answer:')[-1].strip()\n",
        "    sections['answer_letter'] = answer_part[0] if answer_part else ''\n",
        "\n",
        "    content = text.split('SAT READING COMPREHENSION TEST')[-1].split('Answer:')[0]\n",
        "\n",
        "    blocks = [b.strip() for b in content.split('\\n\\n') if b.strip()]\n",
        "\n",
        "    passage_lines = []\n",
        "    for line in blocks:\n",
        "        if line.startswith('Question'):\n",
        "            break\n",
        "        passage_lines.append(line)\n",
        "    sections['passage'] = '\\n'.join(passage_lines).strip()\n",
        "\n",
        "    for block in blocks:\n",
        "        if block.startswith('Question'):\n",
        "            q_part = block.split(')', 1) if ')' in block else (block, '')\n",
        "            sections['question'] = q_part[-1].split('\\n')[0].strip()\n",
        "            sections['choices'] = [line.strip() for line in block.split('\\n')[1:]\n",
        "                                   if line.startswith(('A)', 'B)', 'C)', 'D)'))]\n",
        "    return sections\n",
        "\n",
        "def map_answer(text, letter):\n",
        "    sections = extract_sections(text)\n",
        "    for choice in sections['choices']:\n",
        "        if choi.startswith(f'{letter}'):\n",
        "            return choice\n",
        "    return letter"
      ],
      "metadata": {
        "id": "DD9cHCSjyAty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LLAMA3_SYSTEM_PROMPT = \"\"\"You are ahelpful AI assistant developed by Meta. Respond safely andaccurately.\"\"\"\n",
        "\n",
        "def generate_prompt(text, answer_letter):\n",
        "    sections = extract_sections(text)\n",
        "    choices_text =\"\\n\".join(sections[’choices’])\n",
        "\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": LLAMA3_SYSTEM_PROMPT\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Read the passage and answer the question.\n",
        "\n",
        "                ### Passage:\n",
        "                {sections[\"passage\"]}\n",
        "\n",
        "                ### Question:\n",
        "                {sections[\"question\"]}\n",
        "\n",
        "                ### Choices:\n",
        "                {choices_text}\n",
        "\n",
        "            Respond withONLY the letter and full text of the correct answer.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": map_answer(text, answer_letter)\n",
        "        }\n",
        "    ]"
      ],
      "metadata": {
        "id": "Tj8JQPxz2vdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_tokenize_prompt(user_input, answer):\n",
        "    try:\n",
        "        full_prompt = generate_prompt(user_input, answer)\n",
        "\n",
        "        prompt_str = tokenizer.apply_chat_template(\n",
        "            full_prompt,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "\n",
        "        tokenized = tokenizer(\n",
        "            prompt_str,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=1024,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = tokenized[\"input_ids\"][0]\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": tokenized[\"attention_mask\"][0],\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "    except Exceptionas e:\n",
        "        print(f\"Error processing sample: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "CsV7HzpW3zH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_samples = []\n",
        "for sample in tqdm(data[\"train\"]):\n",
        "    try:\n",
        "        processed_text = sample[\"text\"].replace(\"SAT READING COMPREHENSION TEST\", \"\").strip()\n",
        "        processed_answer = map_answer(sample[\"text\"], sample[\"answer\"].strip())\n",
        "        tokenized_sample = generate_and_tokenize_prompt(processed_text, processed_answer)\n",
        "\n",
        "        if tokenized_sample is not None:\n",
        "            training_samples.append(tokenized_sample)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping invalid sample: {e}\")\n",
        "\n",
        "training_samples = [s for s in training_samples if s is not None]\n",
        "train_samples, val_samples = train_test_split(training_samples, test_size=0.1,\n",
        "                                              random_state=42)\n",
        "train_dataset = Dataset.from_list(train_samples)\n",
        "eval_dataset = Dataset.from_list(val_samples)"
      ],
      "metadata": {
        "id": "-E3iYDNe3zFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune LLM"
      ],
      "metadata": {
        "id": "J7Uww-4l4pTs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mT-35ZGi3y-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogLossCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs is not None and \"loss\" in logs:\n",
        "            print(f\"Step {state.global_step}- Loss: {logs['loss']:.4f}\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    save_total_limit=3,\n",
        "    logging_steps=10,\n",
        "    output_dir=\"llama3-8b-sat-reading\",\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.05,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        "    pad_to_multiple_of=8\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[LogLossCallback()]\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.enable_input_require_grads()\n",
        "model = torch.compile(model)\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "n8UMwDV538Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save checkpoints"
      ],
      "metadata": {
        "id": "dNxId5v65KMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"trained-model\")\n",
        "PEFT_MODEL = \"your_huggingface_user_name/instructionTuning-llama-3-1-8B-SAT-reading-solver\"\n",
        "model.push_to_hub(PEFT_MODEL, use_auth_token=True)"
      ],
      "metadata": {
        "id": "a57nPgMJ3y8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Inference"
      ],
      "metadata": {
        "id": "hNuVUSAs5QnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PEFT_MODEL = \"your_huggingface_user_name/instructionTuning-llama-3-1-8B-SAT-reading-solver\"\n",
        "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = PeftModel.from_pretrained(model, PEFT_MODEL)"
      ],
      "metadata": {
        "id": "jWIbqKPl3y5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=64,\n",
        "    temperature=0.0,\n",
        "    top_p=1.0,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.0,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "def predict(text):\n",
        "    messages = format_test_prompt(text)\n",
        "\n",
        "    prompt_text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=False\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return extract_answer(output_text)\n",
        "\n",
        "for i in range(10):\n",
        "    print(\"=\"*100)\n",
        "    sample = data[\"test\"][i]\n",
        "    input_text = sample[\"text\"]\n",
        "    true_answer = sample[\"answer\"].strip()\n",
        "\n",
        "    predicted_answer = predict(input_text)\n",
        "\n",
        "    true_answer_full = map_answer(input_text, true_answer)\n",
        "    pred_choice = extract_choice_letter(predicted_answer)\n",
        "    true_choice = extract_choice_letter(true_answer_full)\n",
        "\n",
        "    print(f\"### Sample {i+1}\")\n",
        "    print(f\"[Question]\\n{extract_sections(input_text)[\"question\"]}\")\n",
        "    print(f\"[Choices]\\n{extract_sections(input_text)[\"choices\"]}\")\n",
        "    print(f\"\\n[Model Prediction]\\n{predicted_answer}\")\n",
        "    print(f\"\\n[Ground Truth]\\n{true_answer_full}\")\n",
        "    print(f\"\"\"\\nResult: {\"CORRECT\" if pred_choice == true_choice else \"INCORRECT\"}\"\"\")\n",
        "    print(\"=\"*100 + \"\\n\")"
      ],
      "metadata": {
        "id": "CWZQslan5qbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-3Bjv1xM5sZm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}